Computer Architecture and System Design


Computer Disk Storage

Computer disk storage holds the primary data and is classified into two main types: HDD (Hard Disk Drive) and SSD (Solid State Drive). Disk storage is non-volatile, meaning data is maintained even without power. It contains the operating system, applications, and user files. While SSDs are more expensive, they offer significantly faster read speeds compared to HDDs.
Random Access Memory (RAM)

RAM is the primary memory that stores data from applications currently in use. When an application is running, its variables, intermediate computations, and runtime stack are stored in RAM because of its fast read and write access. However, RAM is volatile, meaning it requires power to retain data. Once the computer is turned off or restarted, all data in RAM is lost.

Since RAM may not always be sufficient, a cache memory is used to improve performance. The cache is smaller than RAM (typically measured in megabytes) but provides much faster access times. The CPU first checks the L1 cache, then moves to L2 and L3 caches, and finally RAM if the data is not found in the cache. The purpose of caching is to reduce the average time required to retrieve data.
Central Processing Unit (CPU)

The CPU is the brain of the computer, responsible for fetching, decoding, and executing instructions. When a program is executed, the CPU processes the instructions by reading and writing data to RAM, disk storage, and cache. Before execution, code must first be compiled into machine code by a compiler.
Motherboard

The motherboard acts as the main circuit board, providing pathways for data to flow between components, ensuring communication between the CPU, RAM, disk storage, and peripheral devices.
High-Level Architecture of a Production-Ready Application

A CI/CD pipeline automates the deployment process, moving code from a repository to the production server. This is typically configured using tools such as Jenkins or GitHub Actions.

Once in production, the application must handle multiple user requests efficiently. This is achieved using load balancers and reverse proxies, which distribute user requests across multiple servers to prevent any single server from becoming overloaded.

For data storage, the application interacts with dedicated database servers, ensuring data is managed separately from the main application logic.

To maintain system reliability, logging and monitoring systems (e.g., Sentry and PM2) continuously track application activity. When an issue occurs, alerts can be sent via an alerting service (such as Slack integration), notifying the development team.

The typical debugging workflow involves three key steps:

    Identify the issue through logs and monitoring.

    Replicate the problem in a staging or testing environment.

    Apply a hotfix and deploy it to production.


Pillars of System Design

A well-designed system should focus on the following key principles:

    Scalability: The system should support a growing number of users and increased workload without degradation in performance.

    Maintainability: The system should be structured so that future developers can easily modify and improve it.

    Efficiency: Resources should be used optimally to ensure cost-effectiveness and high performance.

    Reliability: The system should function correctly and consistently under normal conditions, minimizing downtime and failures.

By adhering to these principles, a production-ready system can be built to handle large-scale operations effectively.

Data Management and System Design
Moving Data

Ensuring data can be transferred seamlessly across systems requires optimizing for speed and security. Efficient data movement is crucial for maintaining system performance and preventing bottlenecks.
Storing Data

Data storage strategies depend on access patterns, indexing techniques, and backup solutions. The goal is to ensure data is securely stored and readily available when needed. Proper storage planning prevents data loss and improves query performance.
Data Transformation

Transforming raw data into meaningful information is a critical aspect of data management. This includes data cleaning, aggregation, and structuring to make insights actionable.
ACID Properties in Databases

ACID is a set of principles ensuring reliable database transactions:

    Atomicity: If any part of a transaction fails, the entire transaction is rolled back.

    Consistency: The database transitions from one valid state to another, maintaining integrity.

    Isolation: Transactions are executed independently, preventing conflicts.

    Durability: Once a transaction is committed, it remains permanent, even in case of system failures.


CAP Theorem

CAP theorem highlights the trade-offs in a distributed system and states that a system can only guarantee two out of three properties:

    Consistency: Ensures that all nodes in the system have the same data at any given time. Any update on one node is reflected in all others.

    Availability: Ensures that the system remains operational and responsive to requests.

    Partition Tolerance: Ensures that the system can continue functioning even if some nodes are unreachable.

Every system design decision involves trade-offs. For example, optimizing read performance may come at the cost of write performance, and increasing speed may introduce additional complexity. The goal is not always to find the perfect solution, but to make informed compromises based on system priorities.
System Availability

Availability measures a system's performance and reliability, often expressed as a percentage:

    99.9% availability → approximately 8.76 hours of downtime per year.

    99.999% availability → approximately 5 minutes of downtime per year.

Availability is tracked through uptime and downtime metrics.

    Service Level Objectives (SLOs) define performance goals, such as maintaining a 300ms response time and 99.9% uptime.

    Service Level Agreements (SLAs) are formal agreements specifying expected latency and availability for end users.


System Resilience

Resilience ensures a system can recover from failures through strategies like redundant backups or graceful degradation. It is evaluated using:

    Reliability: The system consistently operates correctly.

    Fault Tolerance: The system's ability to handle failures without downtime.

    Redundancy: Backup components that can take over in case of failures.

System Performance

Performance is measured through speed and throughput:

    Throughput: The amount of data a system can handle over time (e.g., requests per second, database queries per second, or bytes transferred per second).

    Latency: The time taken for a request to receive a response.

Optimizing for one may negatively impact the other. For example, batching requests can improve throughput but may increase latency.
System Design Considerations

Designing scalable and secure systems is a long-term investment. Unlike application code, redesigning an entire system is complex and costly, making upfront planning critical.


Networking Basics

How Computers Communicate

Computers exchange data over a network using IP addresses and networking protocols.

    IPv4: A 32-bit address format supporting 4 billion unique addresses.

    IPv6: A 64-bit address format supporting 304 trillion unique addresses, designed to address IPv4 limitations.

Every network message is sent with an IP header, following the Internet Protocol (IP) to ensure proper data routing across the internet.

By understanding these concepts, engineers can build scalable, reliable, and efficient systems.


Transport layer

This is where TCP and UDP come into play

TCP (Transmission Control Protocol) operates at the transport layer and ensures reliable communication, checking that not only the packet arrives, but that nothing is missing. Each TCP data header has a specified source port number, destination port number, 
sequence number to make sure that the packets arrive on sequence, acknowledgement number, flags and checksums to make sure that no data is lost.

Processes like the 3 way handshake establishes reliable connection between two devices before packets are sent

In contrast, there is UDP (User Datagram Protocol). It is faster but less reliable. There is not established connection between the two sources and it does not guarantee the order and delivery of data packets.But this is 
what makes UDP prefered in time crucial activities where data loss can be accepted 


DNS (Domain name System) is the internet's phonebook, translating internet domain names to ip addresses that computers can understand. When you enter a url into your browser, the browser then sends a dns query to the
dns server to find the corresponding ip address, allowing it to establish the connection and retrive the webpage. Domain name site like godaddy are creditted by ICANN to sell domain names to the public

DNS records
AAAA records map a domain name to an IPv6 address while A records map a domani anme to an IPv4 address


Networking Infrastructure

Devices on the network have either public or private IP addresses. Public address are unique across the internet while private ip addresses are unique across the local network. The can also be static or dynamic, where a 
dynamic ip address is used for residential internet connection. 

LANs (Local Area Networks) are protected by firewalls which monitor and control incoming and outgoing traffic. Ports, together with an ip address, creates a unique identifier for a network service. 


Application layer

HTTP (Hyper text transfer protocol) is built on tcp/ip. It is a communications protocol but with no memory, meaning each interaction is seperate. Server does not store any context in each request. Each request contains
status codes, request urls, methods, request payload and request body

Success codes: 200 are success codes, 300 are redirection codes (resource has relocated), 400 are client error codes(bad syntax or request cannot be fulfilled), 500 server error codes (server error codes)

HTTP also contain methods on request: 
Get: getting data from server
Post: creating data on the server 
Put: updates a resource completely
Patch: updates a portion of the data
Delete: delete a resource

For real time updates, websockets are used to provide two way communication channel between two nodes. This is usually used for chat room messaging platforms, live sports updates or stock market movements

SMTP is the standard email protocol for email transmission over the internet. IMAP or POP3 are used for receiving them. IMAP is used to retrieve emails from servers and edit them. It is required for retriving emails from 
multiple devices. POP3 is used for downloading email from a server to a local client. This is usually used when emails are read on a single device. 

File transfer protocols. FTP: is used for data maintenance and large data transfers. It is used to upload files from client to server, to other servers and backing up files. SSH is also a way to operate servers remotely and 
securely on an unsecure network

Realtime communication protocols like WebRTC, which enables browser to browser applications for voice calling, video chatting and file sharing. MQTT (Message Queuing Telemetry Transport) is a light weight messaging protocol 
for devices with limited processing power and low bandwidth such as IoT devices. AMQP (Advanced Messaging Queuing Protocol) is for message-oriented middleware.

RPC (Remote Procedure Call) allows one computer to execute code on another computer remotely. It executes functions like it is a local call but in reality the function is in a remote machine. It abstracts the details
of interacting with a server far away and makes it seem to the user that the interactions are all local in nature. 


API Design

In api design, we are concerned about the input and the outputs, with the problem the definition of how the crud operations are exposed to the user interface. Create, read, update and delete. To create a product, you 
send a post request to the create api end point with the data in the request body. To read, we send a get request. For updating, we either send a put or a patch request and for deleting we send a delete request. 

Another portion of the design is to decide which protocol to use, either http, websockets or any other protocols as well as the data transport mechanism which can be json or xml

Apart from rest, there are different type of paradiagms like graphql and gRPC

REST (representational state transfer)
- it is stateless, this means that each request must contain enough context for the request to be processed
- Uses the standard HTTP methods: GPPPD
- Compatible with most browsers and applications
- use JSON for data exchange
Downsides: might lead to overfetching or underfetching as more end points are needed for access specific data 

GRAPHQL
- avoids overfetching and underfetching for clients to request exactly what they need
- they have strongly typed queries
Downsides: these strongly typed queries might impace server performance, all request are sent only in post request and responses only in 200 (error details are in the response body)

gRPC (google remote procedure call)
- built on HTTP/2
- uses multiplexing and server push
- uses protocol buffers
- efficient and useful for microservices
Downsides: less human readable than json and requires http 2.0 to operate

Api urls have references to other relationships if the query has one and will commonly contain other params as well. A well designed get request should be idempotent. When modifying endpoints, backward compatibility must be maintain

Rate limits can also be added to prevent a single user from sending too many request to the api. Settings CORS settings (Cross origin resource sharing) can control which domains will have access to your api, preventing 
unwanted cross site interactions


Caching and CDN

Caching is used to increase the performance and efficiency of a system. It requires storing a copy of data in a temporary storage so that future querying of that data can be served faster. When a requested object is in 
the cache, it is a cache hit, while a cache miss is when the resource is not in the cache and must be retrived via a call to the server. The cache ratio is the ratio betwee the number of cache hits and cache hits + cache misses.

Where is caching done?
- Browser caching: website resources are stored in a users local computer. When the user reloads the site, the browser can load everything from the cache instead of refetching it from the server again. For each network response
there are cache control attributes in the headers that state how long the object should be held in the cache. There also is a X-Cache header that specifies whether the cache hits or misses

- Server caching: server data that are frequently retrieved are cache to reduce the expense of refetching them again (database queries are one such example of an expensive query). The server caches are stored on server or
in a seperate cache server on disk or memory (redis). Typically a server will check the cache server, if the data is not there, then it will retrieve it from the database. After receiving the data, the server will then serve
the data to the user and place it in the cache for future reference. 
    - Write around cache is when every write operation is performed both in the cache and the database ast the same time. This is to ensure the cache is always
updated but leads to high write latency. 
    - Write through cache is when writes are only performed in the primary storage and the cache is written only when the object is requested. This is more efficient but leads to higher cache miss
    - Write back cache is when write are done to the cache first, then to the database at a later time. This improves write performance but risks the loss of data if the cache server crashes
    - When the cache is full, there are several eviction policies that are used to remove items from the cache. These are LRU, LFU and FIFO

- Database Caching: It is often done within the database itself, or on a seperate layer like redis or memcache. A query will first go to the cache, only if it is a cache miss will the query then be returned to the database, which will then 
store the data in the cache as well

- CDNs (Content Delivery Networks): Are a collection of servers that are distributed geographically to serve static content or image/video files. The pull-based type of arrangement is when the content is a cache miss and the CDN will pull the 
from the origin server. It is ideal for websites with alot of static content and requires less active management. The push-based arrangement is when content is uploaded to the origin server which then pushes the content to the CDNs. This is 
useful for large files that are infrequently updated but need to be updated quickly. More active management is needed. 
CDNs provide high availability and performance for users across different geographies. It also reduces the load on the origin server. However, an origin server should be used if the dynamic content changes frequently or is personalised to the
user. If the tasks handled require real time processing or that there are complex server side logic required, then an origin server is still prefered   
CDNs benefit: Reduces latency, high availability, and improved security
Cache benefit: Reduce latency, lower server load, and improves UX


Proxies
Proxy server is the intermediary between the client being served the resource and the server serving the resource. They help in caching resources, anonymising requests and load balancing

Types of proxy servers:
Forward proxy = sits in front of clients and is used to sent request to other servers on the internet. It is usually used in internal networks to control internet access
Reverse proxy = sits in front of one or more web servers. It is used for load balancing and security
Open proxy = allows anyone to access the proxy. It is used for annoymising web browsing and bypassing content restrictions
Transparent proxy = content is visible to the cilent. It is used for caching and content filtering
Annoymous proxy = used for annoymous browsing
Distorting proxy = incorrect IP misinformation
High Anonymity proxy = max anonymity with no headers

When the client sends a request via a proxy, the proxy decides on whether to allow the request, modify it or block it. Something commonly done is that the proxy server will hide the client's ip address so that it cannot be identified by the server
Example use case: controlling of internet use in companies, caching, anonymising web access

Reverse proxies sit in front of servers and intercept request before they reach the server. The reverse proxy hides the servers identity. It also helps to distribute the load between multiple servers, making sure that no single server becomes overwhelmed. They also can 
compress data, cache files and manange ssl encryption. CDNs are also a type of reverse proxies, where they deliver static content based on the geographical location of the user. Firewalls are also one other example