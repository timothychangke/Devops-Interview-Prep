Computer Architecture and System Design


Computer Disk Storage

Computer disk storage holds the primary data and is classified into two main types: HDD (Hard Disk Drive) and SSD (Solid State Drive). Disk storage is non-volatile, meaning data is maintained even without power. It contains the operating system, applications, and user files. While SSDs are more expensive, they offer significantly faster read speeds compared to HDDs.
Random Access Memory (RAM)

RAM is the primary memory that stores data from applications currently in use. When an application is running, its variables, intermediate computations, and runtime stack are stored in RAM because of its fast read and write access. However, RAM is volatile, meaning it requires power to retain data. Once the computer is turned off or restarted, all data in RAM is lost.

Since RAM may not always be sufficient, a cache memory is used to improve performance. The cache is smaller than RAM (typically measured in megabytes) but provides much faster access times. The CPU first checks the L1 cache, then moves to L2 and L3 caches, and finally RAM if the data is not found in the cache. The purpose of caching is to reduce the average time required to retrieve data.
Central Processing Unit (CPU)

The CPU is the brain of the computer, responsible for fetching, decoding, and executing instructions. When a program is executed, the CPU processes the instructions by reading and writing data to RAM, disk storage, and cache. Before execution, code must first be compiled into machine code by a compiler.
Motherboard

The motherboard acts as the main circuit board, providing pathways for data to flow between components, ensuring communication between the CPU, RAM, disk storage, and peripheral devices.
High-Level Architecture of a Production-Ready Application

A CI/CD pipeline automates the deployment process, moving code from a repository to the production server. This is typically configured using tools such as Jenkins or GitHub Actions.

Once in production, the application must handle multiple user requests efficiently. This is achieved using load balancers and reverse proxies, which distribute user requests across multiple servers to prevent any single server from becoming overloaded.

For data storage, the application interacts with dedicated database servers, ensuring data is managed separately from the main application logic.

To maintain system reliability, logging and monitoring systems (e.g., Sentry and PM2) continuously track application activity. When an issue occurs, alerts can be sent via an alerting service (such as Slack integration), notifying the development team.

The typical debugging workflow involves three key steps:

    Identify the issue through logs and monitoring.

    Replicate the problem in a staging or testing environment.

    Apply a hotfix and deploy it to production.


Pillars of System Design

A well-designed system should focus on the following key principles:

    Scalability: The system should support a growing number of users and increased workload without degradation in performance.

    Maintainability: The system should be structured so that future developers can easily modify and improve it.

    Efficiency: Resources should be used optimally to ensure cost-effectiveness and high performance.

    Reliability: The system should function correctly and consistently under normal conditions, minimizing downtime and failures.

By adhering to these principles, a production-ready system can be built to handle large-scale operations effectively.

Data Management and System Design
Moving Data

Ensuring data can be transferred seamlessly across systems requires optimizing for speed and security. Efficient data movement is crucial for maintaining system performance and preventing bottlenecks.
Storing Data

Data storage strategies depend on access patterns, indexing techniques, and backup solutions. The goal is to ensure data is securely stored and readily available when needed. Proper storage planning prevents data loss and improves query performance.
Data Transformation

Transforming raw data into meaningful information is a critical aspect of data management. This includes data cleaning, aggregation, and structuring to make insights actionable.
ACID Properties in Databases

ACID is a set of principles ensuring reliable database transactions:

    Atomicity: If any part of a transaction fails, the entire transaction is rolled back.

    Consistency: The database transitions from one valid state to another, maintaining integrity.

    Isolation: Transactions are executed independently, preventing conflicts.

    Durability: Once a transaction is committed, it remains permanent, even in case of system failures.


CAP Theorem

CAP theorem highlights the trade-offs in a distributed system and states that a system can only guarantee two out of three properties:

    Consistency: Ensures that all nodes in the system have the same data at any given time. Any update on one node is reflected in all others.

    Availability: Ensures that the system remains operational and responsive to requests.

    Partition Tolerance: Ensures that the system can continue functioning even if some nodes are unreachable.

Every system design decision involves trade-offs. For example, optimizing read performance may come at the cost of write performance, and increasing speed may introduce additional complexity. The goal is not always to find the perfect solution, but to make informed compromises based on system priorities.
System Availability

Availability measures a system's performance and reliability, often expressed as a percentage:

    99.9% availability → approximately 8.76 hours of downtime per year.

    99.999% availability → approximately 5 minutes of downtime per year.

Availability is tracked through uptime and downtime metrics.

    Service Level Objectives (SLOs) define performance goals, such as maintaining a 300ms response time and 99.9% uptime.

    Service Level Agreements (SLAs) are formal agreements specifying expected latency and availability for end users.


System Resilience

Resilience ensures a system can recover from failures through strategies like redundant backups or graceful degradation. It is evaluated using:

    Reliability: The system consistently operates correctly.

    Fault Tolerance: The system's ability to handle failures without downtime.

    Redundancy: Backup components that can take over in case of failures.

System Performance

Performance is measured through speed and throughput:

    Throughput: The amount of data a system can handle over time (e.g., requests per second, database queries per second, or bytes transferred per second).

    Latency: The time taken for a request to receive a response.

Optimizing for one may negatively impact the other. For example, batching requests can improve throughput but may increase latency.
System Design Considerations

Designing scalable and secure systems is a long-term investment. Unlike application code, redesigning an entire system is complex and costly, making upfront planning critical.


Networking Basics

How Computers Communicate

Computers exchange data over a network using IP addresses and networking protocols.

    IPv4: A 32-bit address format supporting 4 billion unique addresses.

    IPv6: A 64-bit address format supporting 304 trillion unique addresses, designed to address IPv4 limitations.

Every network message is sent with an IP header, following the Internet Protocol (IP) to ensure proper data routing across the internet.

By understanding these concepts, engineers can build scalable, reliable, and efficient systems.


Transport layer

This is where TCP and UDP come into play

TCP (Transmission Control Protocol) operates at the transport layer and ensures reliable communication, checking that not only the packet arrives, but that nothing is missing. Each TCP data header has a specified source port number, destination port number, 
sequence number to make sure that the packets arrive on sequence, acknowledgement number, flags and checksums to make sure that no data is lost.

Processes like the 3 way handshake establishes reliable connection between two devices before packets are sent

In contrast, there is UDP (User Datagram Protocol). It is faster but less reliable. There is not established connection between the two sources and it does not guarantee the order and delivery of data packets.But this is 
what makes UDP prefered in time crucial activities where data loss can be accepted 


DNS (Domain name System) is the internet's phonebook, translating internet domain names to ip addresses that computers can understand. When you enter a url into your browser, the browser then sends a dns query to the
dns server to find the corresponding ip address, allowing it to establish the connection and retrive the webpage. Domain name site like godaddy are creditted by ICANN to sell domain names to the public

DNS records
AAAA records map a domain name to an IPv6 address while A records map a domani anme to an IPv4 address


Networking Infrastructure

Devices on the network have either public or private IP addresses. Public address are unique across the internet while private ip addresses are unique across the local network. The can also be static or dynamic, where a 
dynamic ip address is used for residential internet connection. 

LANs (Local Area Networks) are protected by firewalls which monitor and control incoming and outgoing traffic. Ports, together with an ip address, creates a unique identifier for a network service. 